{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":2430275,"sourceType":"datasetVersion","datasetId":1470377},{"sourceId":4089743,"sourceType":"datasetVersion","datasetId":2419864},{"sourceId":10183021,"sourceType":"datasetVersion","datasetId":6290509},{"sourceId":10183189,"sourceType":"datasetVersion","datasetId":6290636},{"sourceId":10186789,"sourceType":"datasetVersion","datasetId":6293258},{"sourceId":10187143,"sourceType":"datasetVersion","datasetId":6293524},{"sourceId":10188323,"sourceType":"datasetVersion","datasetId":6294448}],"dockerImageVersionId":30235,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install vncorenlp","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from vncorenlp import VnCoreNLP\n\nvncorenlp = VnCoreNLP(\"../input/vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner,parse\", max_heap_size='-Xmx2g')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-13T08:37:53.615664Z","iopub.execute_input":"2024-12-13T08:37:53.616022Z","iopub.status.idle":"2024-12-13T08:38:19.101186Z","shell.execute_reply.started":"2024-12-13T08:37:53.615990Z","shell.execute_reply":"2024-12-13T08:38:19.100341Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\nfrom vncorenlp import VnCoreNLP\nimport nltk\n\n\nnltk.download('punkt')\n\n\ndef split_into_sentences(text):\n    sentences = nltk.sent_tokenize(text)\n    return [sentence for sentence in sentences if sentence.strip()]\n\n\ndef apply_ner_with_tokenize(text):\n    ner_results = []\n    tokenized_text = []  \n    try:\n        result = vncorenlp.ner(text)\n        for sentence in result:\n            filtered_entities = [entity for entity in sentence if entity[1] != \"O\"]\n            ner_results.extend(filtered_entities)\n            tokenized_text.append(\" \".join([token[0] for token in sentence]))\n    except Exception as e:\n        print(f\"Error during NER: {e}\")\n    return ner_results, \" \".join(tokenized_text)\n\ndef detokenize(text):\n    \"\"\"\n    Xử lý lại văn bản được tokenize để loại bỏ lỗi khoảng trắng không cần thiết.\n    \"\"\"\n    text = text.replace(\" _ \", \"_\")  \n    text = text.replace(\" _\", \"_\").replace(\"_ \", \"_\")  \n    text = text.replace(\" ,\", \",\").replace(\" .\", \".\")  \n    text = text.replace(\" :\", \":\").replace(\" ;\", \";\")  \n    text = text.replace(\"( \", \"(\").replace(\" )\", \")\")  \n    text = text.replace(\" '\", \"'\").replace(\"' \", \"'\")  \n    text = text.replace(\" - \", \"-\")  \n    return text\n\n\ndef process_entities_and_tokenize(index, text):\n    sentences = split_into_sentences(text)\n    ner_results = []\n    tokenized_text = []\n    for sentence in sentences:\n        if sentence.strip():  \n            ner_result, tokenized_sentence = apply_ner_with_tokenize(sentence)\n            ner_results.extend(ner_result)\n            tokenized_text.append(tokenized_sentence)\n    \n    entities = []\n    if ner_results:\n        i = 0\n        while i < len(ner_results):\n            entity, tag = ner_results[i]\n            if tag.startswith(\"B-\"):\n                current_entity = [entity]\n                current_tag = tag[2:]\n                i += 1\n                while i < len(ner_results) and ner_results[i][1] == f\"I-{current_tag}\":\n                    current_entity.append(ner_results[i][0])\n                    i += 1\n                \n                entities.append((index, \" \".join(current_entity), current_tag))\n            else:\n                i += 1\n    \n  \n    unique_entities = list(set(entities))\n    \n    tokenized_text = detokenize(\" \".join(tokenized_text))\n    return unique_entities, tokenized_text\n\n\ndef process_file(file_name, output_tokenized, output_entities):\n    try:\n        \n        df = pd.read_csv(file_name)\n\n        columns = ['Số thứ tự bài báo', 'Thực thể', 'Loại thực thể']\n        df_entity = pd.DataFrame(columns=columns)\n\n    \n        for index, row in df.iterrows():\n            text = str(row[\"content\"])\n            entities, tokenized_text = process_entities_and_tokenize(row[\"idx\"], text)\n            \n          \n            df.at[index, \"content\"] = tokenized_text\n            \n         \n            for index1, entity, entity_type in entities:\n                new_row = {'Số thứ tự bài báo': index1, 'Thực thể': entity, 'Loại thực thể': entity_type}\n                df_entity = pd.concat([df_entity, pd.DataFrame([new_row])], ignore_index=True)\n\n        \n        df.to_csv(output_tokenized, encoding=\"utf-8-sig\", index=False)\n        df_entity.to_csv(output_entities, encoding=\"utf-8-sig\", index=False)\n        print(f\"Processed {file_name}: Output saved to {output_tokenized} and {output_entities}\")\n\n    except Exception as e:\n        print(f\"Error processing file {file_name}: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"num_sample = 3","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfor i in range(1, num_sample + 1):\n    file_name = f\"/kaggle/working/train_preprocessing1_{i}.csv\"\n    output_tokenized = f\"/kaggle/working/train_ner_{i}.csv\"\n    output_entities = f\"/kaggle/working/train_entities_{i}.csv\"\n    process_file(file_name, output_tokenized, output_entities)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nfor i in range(1, num_sample + 1):\n    file_name = f\"/kaggle/working/test_preprocessing1_{i}.csv\"\n    output_tokenized = f\"/kaggle/working/test_ner_{i}.csv\"\n    output_entities = f\"/kaggle/working/test_entities_{i}.csv\"\n    process_file(file_name, output_tokenized, output_entities)\n\n\nvncorenlp.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\nfile_path = ''\n\nwith open(file_path, 'r', encoding='utf-8') as f:\n    stopwords = [word.strip().lower() for word in f.readlines() if word.strip()]\n\ndef remove_stopwords(text, stopwords):\n    text = text.replace(',', ' , ').replace('.', ' . ')\n    words = text.split()\n    filtered_words = [word for word in words if word.lower() not in stopwords]\n    return ' '.join(filtered_words)\n\ndf['content'] = df['content'].apply(lambda x: preprocess_and_remove_stopwords(str(x), stopwords))\n\nprint(df['content']\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}